{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from modules.config import AppConfig\n",
    "from modules.utils.handle_directories import load_pickle, get_pickle_file_path, get_pickle_folder, get_figure_dir, get_train_folders\n",
    "from modules.utils.handle_pytorch import load_model\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy import interp\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import roc_curve, balanced_accuracy_score, roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix\n",
    "from scipy.stats import kurtosis\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = AppConfig.targets\n",
    "target_indices = list(range(len(targets)))\n",
    "if len(target_indices) > 1:\n",
    "    target_indices.append(None)\n",
    "mimic_version = 4\n",
    "time_step_id = 14\n",
    "seeds = [0]\n",
    "n_folds = 1 # AppConfig.k_folds\n",
    "\n",
    "AppConfig.device = 'cpu'\n",
    "\n",
    "balance_data = True\n",
    "oversample = False\n",
    "\n",
    "debug = False\n",
    "save_unimportant_figures = False\n",
    "\n",
    "best_model_dir, final_model_dir, logs_dir = get_train_folders(undersample=balance_data, oversample=oversample)\n",
    "model_dir = best_model_dir\n",
    "figure_format = 'png'\n",
    "figure_dir = get_figure_dir(undersample=balance_data, oversample=oversample)\n",
    "\n",
    "if not os.path.exists(figure_dir):\n",
    "    os.makedirs(figure_dir)\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "print(f'{figure_dir=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if targets == 'MI':\n",
    "    my_cmap = ListedColormap(sns.color_palette(\"Reds\", 150))\n",
    "    color_list = sns.color_palette(\"Reds\", 14)\n",
    "    color_list_reduced = sns.color_palette(\"Reds\", 7) \n",
    "elif targets == 'SEPSIS':\n",
    "    my_cmap = sns.cubehelix_palette(14, start=2, rot=0, dark=0.25, light=.95, as_cmap=True)\n",
    "    color_list = sns.cubehelix_palette(14, start=2, rot=0, dark=0.15, light=.8)\n",
    "    color_list_reduced = sns.cubehelix_palette(7, start=2, rot=0, dark=0.15, light=.8)\n",
    "else:\n",
    "    my_cmap= sns.cubehelix_palette(14, as_cmap=True)\n",
    "    color_list = sns.cubehelix_palette(14)\n",
    "    color_list_reduced = sns.cubehelix_palette(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PICKLE LOADS\n",
    "def load_pickle_file(file_name, target, folder):\n",
    "    print(get_pickle_file_path(file_name, target, folder))\n",
    "    return load_pickle(get_pickle_file_path(file_name, target, folder))\n",
    "\n",
    "def get_model_name(model_type, mimic_version, target, time_step_id, seed, fold=None):\n",
    "    model_name = f'{model_type}_{mimic_version}_{target}_{time_step_id}_{seed}'\n",
    "    if fold is not None:\n",
    "        model_name += f'_{fold}'\n",
    "    return model_name\n",
    "\n",
    "def get_model_dict(model_type, mimic_version, target, time_step_id, seeds, n_folds):\n",
    "    model_dict = dict()\n",
    "    print(f'{model_dir=}')\n",
    "    for model_type in model_types:\n",
    "        model_dict[model_type] = []\n",
    "        for seed in seeds:\n",
    "            for fold in range(n_folds):\n",
    "                model_name = get_model_name(model_type, mimic_version, target, time_step_id, seed, fold)\n",
    "                model_dict[model_type].append(load_model(model_name, path=model_dir))\n",
    "        print(f'Loaded {len(model_dict[model_type])} models of type {model_type}')\n",
    "    return model_dict\n",
    "\n",
    "def get_pickle_dict(file_name, seeds, mimic_version, target, time_step_id):\n",
    "    data_dict = dict()\n",
    "\n",
    "    for seed in seeds:\n",
    "        folder = get_pickle_folder(mimic_version, time_step_id, seed=seed)\n",
    "        print(f'In get_pickle_dict: {folder=}')\n",
    "        pickle = load_pickle_file(file_name, target, folder)\n",
    "        data_dict[seed] = pickle\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def print_param_count(model_dict, count_per_layer=False):\n",
    "    for model_type in model_dict.keys():\n",
    "        model = model_dict[model_type][0]\n",
    "        if 'xgb' not in model_type.lower():\n",
    "            num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(f'{model_type} number of Parameters: {num_param}')\n",
    "            if count_per_layer:\n",
    "                for name, p in model.named_parameters():\n",
    "                    if p.requires_grad:\n",
    "                        print(f'\\t{name}: {p.numel()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_rows(mimic_version, time_step_id, seed, targets):\n",
    "    folder = get_pickle_folder(mimic_version, time_step_id, seed)\n",
    "    print(folder, targets)\n",
    "    data = load_pickle_file('train_data', targets, folder)\n",
    "    print(data.shape)\n",
    "    re = data.reshape(-1, data.shape[2])\n",
    "    print(re.shape)\n",
    "    uniques = np.unique(re, axis=0)\n",
    "    print(uniques.shape)\n",
    "    \n",
    "count_unique_rows(mimic_version, time_step_id, seeds[0], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_data_distribution(mimic_version, time_step_id, seeds, target, targets, save_fig = True):\n",
    "    labels = [\"Total data\", \"Training set\", \"Test set\"]\n",
    "    message = \"The total kurtosis of {0} is {1:.2f} with {2:.2f} % of events occuring between days 0 and 2\"\n",
    "    pos_dict = dict()\n",
    "    total_pos_dict = dict()\n",
    "    n_pos = dict()\n",
    "    n_neg = dict()\n",
    "    target_index = targets.index(target)\n",
    "    \n",
    "    for label in labels:\n",
    "        pos_dict[label] = []\n",
    "        total_pos_dict[label] = []\n",
    "        n_pos[label] = []\n",
    "        n_neg[label] = []\n",
    "        \n",
    "    for seed in seeds:\n",
    "        folder = get_pickle_folder(mimic_version, time_step_id, seed, balanced=balance_data)\n",
    "        print(f'Loaded Datasets from: {folder}')\n",
    "        y_train = load_pickle_file('train_targets', targets, folder)[:,:,target_index]\n",
    "        y_test = load_pickle_file('test_targets', targets, folder)[:,:,target_index]\n",
    "        split_sets = [y_train, y_test]\n",
    "        y_total = np.concatenate(split_sets, axis=0)\n",
    "        \n",
    "        n_time_steps = y_total.shape[1]\n",
    "        sets = [y_total] + split_sets\n",
    "        for y, label in zip(sets, labels):\n",
    "            y_sum = y.sum(axis=1)\n",
    "            n_positives = np.count_nonzero(y_sum)\n",
    "            n_pos[label].append(n_positives)\n",
    "            n_neg[label].append(len(y_sum) - n_positives)\n",
    "            \n",
    "            \n",
    "            y_vector = np.nansum(y, axis=0).squeeze()\n",
    "            # print(message.format(label, kurtosis(y_vector), 100*int(np.sum(y_vector[0:2]))/np.sum(y_vector)))\n",
    "            y_vector *= 100/y_vector.sum()\n",
    "            pos_dict[label].append(y_vector)\n",
    "            \n",
    "            pos_y_total = y[np.nansum(y.squeeze(), axis=1) == 1]\n",
    "            (days_total, value_counts_total) = np.unique(np.nanargmax(pos_y_total, axis=1), return_counts=True)\n",
    "            day_value_dict_total = dict(zip(np.arange(n_time_steps), np.zeros(n_time_steps)))\n",
    "            for i in range(len(days_total)):\n",
    "                day = days_total[i]\n",
    "                day_value_dict_total[day] = value_counts_total[i]\n",
    "            y_vector_total_pos = np.array(list(day_value_dict_total.values()))\n",
    "            y_vector_total_pos = y_vector_total_pos * 100/y_vector_total_pos.sum()\n",
    "            total_pos_dict[label].append(y_vector_total_pos)\n",
    "        \n",
    "    plt.figure(figsize=(18, 12))\n",
    "    sns.set(style=\"white\")\n",
    "    sns.despine(left=True, bottom=True)\n",
    "\n",
    "    ## TOTAL\n",
    "\n",
    "    ### When do people have incidents?\n",
    "\n",
    "    # number of people who are positive on a given day\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('Percentage of patients positive on a given day for each set')\n",
    "    plt.ylabel(f'Percentage of patients on {target}')\n",
    "    plt.xlabel('Day')\n",
    "    \n",
    "    x = np.arange(n_time_steps) + 1\n",
    "    for label in pos_dict.keys():\n",
    "        mean_y = np.mean(pos_dict[label], axis=0)\n",
    "        std_y = np.std(pos_dict[label], axis=0)\n",
    "        upper_y = np.minimum(mean_y + std_y, 100)\n",
    "        lower_y = np.maximum(mean_y - std_y, 0)\n",
    "        plt.plot(x, mean_y, label=label + r' (std: $\\pm$ %0.2f)' % (np.mean(std_y)))\n",
    "        plt.fill_between(x, lower_y, upper_y, color='black', alpha=.2)\n",
    "    plt.legend()\n",
    "        \n",
    "    # first incident - number of people who had their first incident on a given day\n",
    "    x = np.arange(n_time_steps) + 1\n",
    "    plt.subplot(2, 2 ,2)\n",
    "    plt.title('Percentage of patients whose incidence began on a given day for each set')\n",
    "    plt.ylabel(f'Percentage of patients on {target}')\n",
    "    plt.xlabel('Day')\n",
    "    for label in pos_dict.keys():\n",
    "        mean_y = np.mean(total_pos_dict[label], axis=0) \n",
    "        std_y = np.std(total_pos_dict[label], axis=0)\n",
    "        upper_y = np.minimum(mean_y + std_y, 100) \n",
    "        lower_y = np.maximum(mean_y - std_y, 0)\n",
    "        plt.plot(x, mean_y, label=label + r' (std: $\\pm$ %0.2f)' % (np.mean(std_y)))\n",
    "        plt.fill_between(x, lower_y, upper_y, color='black', alpha=.2)\n",
    "        \n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2 ,3)\n",
    "    plt.title(f'Percentage of patients with at least one positive label on {target}')\n",
    "    plt.ylabel('Percentage of patients'.format(target))\n",
    "    plt.xlabel('Set')\n",
    "    y = 0\n",
    "    width = 0.35\n",
    "    for label in pos_dict.keys():\n",
    "        n_total = (n_pos[label][0] + n_neg[label][0])/100\n",
    "        mean_n_pos = np.mean(n_pos[label])/n_total \n",
    "        std_n_pos = np.std(n_pos[label])/n_total \n",
    "        mean_n_neg = np.mean(n_neg[label])/n_total \n",
    "        std_n_neg = np.std(n_neg[label])/n_total \n",
    "        container = plt.bar([y - width/2], mean_n_pos, width=width, color='darkgreen', label='pos')\n",
    "        plt.bar_label(container, [r'%05.2f %%' % (mean_n_pos)])\n",
    "        container = plt.bar([y + width/2], mean_n_neg, width=width, color='darkred', label='neg')\n",
    "        plt.bar_label(container, [r'%05.2f %%' % (mean_n_neg)])\n",
    "        if y == 0:\n",
    "            plt.legend()\n",
    "        y += 1\n",
    "    plt.xticks(range(len(pos_dict.keys())), pos_dict.keys())\n",
    "    \n",
    "    \n",
    "    if save_fig:\n",
    "        plt.savefig(f'{figure_dir}/{target}_Set_Distributions.{figure_format}', \n",
    "                    format=figure_format, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "for target in targets:\n",
    "    plot_data_distribution(mimic_version, time_step_id, seeds, target, targets, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the time component important/ is a FNN sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_prediction_dict(model_dict, input_data):\n",
    "    input_data = torch.tensor(input_data, dtype=torch.float).clone().detach()\n",
    "    input_data_2d = input_data.reshape(-1, input_data.shape[-1])\n",
    "    prediction_dict = dict()\n",
    "    with torch.no_grad():\n",
    "        for model_type in model_dict.keys():\n",
    "            print(f'Creating Predictions for {model_type=}')\n",
    "            prediction_dict[model_type] = []\n",
    "            n_models = len(model_dict[model_type])\n",
    "            for model in model_dict[model_type]:\n",
    "                if 'xgb' in model_type.lower():\n",
    "                    prediction = model.predict(xgb.DMatrix(data=input_data_2d.numpy()))\n",
    "                    prediction = prediction.reshape(input_data.shape[0], input_data.shape[1], 1)\n",
    "                else:\n",
    "                    prediction = model(input_data, apply_activation=True)\n",
    "                    prediction = torch.stack(prediction, dim=2)[:,:,:,0].detach().numpy()\n",
    "                prediction_dict[model_type].append(prediction)\n",
    "    print(f'Number of models/predictions per model type: {len(prediction_dict[model_type])}')\n",
    "    return prediction_dict\n",
    "\n",
    "def plot_curves(predictions, plot_roc, target_dict, mask_dict, labels, target_str, save_fig=True, target_index=None):\n",
    "    if plot_roc:\n",
    "        method = 'ROC curve'\n",
    "    else:\n",
    "        method = 'Precision-Recall curve'\n",
    "    \n",
    "    targets = list(target_dict.values())\n",
    "    masks = list(mask_dict.values())\n",
    "    \n",
    "    mean_x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.axhline(0, color='black')\n",
    "    plt.axvline(0, color='black')\n",
    "    for model_type, label in zip(predictions.keys(), labels):\n",
    "        y_value_list = []\n",
    "        acc_list = []\n",
    "        auc_list = []\n",
    "        n_entries = len(predictions[model_type])\n",
    "        for prediction, y, mask in zip(predictions[model_type], targets * n_entries, masks * n_entries):\n",
    "            if target_index is not None:\n",
    "                mask = mask[:,:,target_index]\n",
    "                y = y[:,:,target_index]\n",
    "                prediction = prediction[:,:,target_index]\n",
    "            y = y[~mask]\n",
    "            prediction = prediction[~mask]\n",
    "            if plot_roc:\n",
    "                x_values, y_values, thresholds = roc_curve(y, prediction)\n",
    "            else:\n",
    "                y_values, x_values, thresholds = precision_recall_curve(y,  prediction)\n",
    "                y_values = y_values[::-1]\n",
    "            y_value_list.append(interp(mean_x, sorted(x_values), y_values))\n",
    "            area_uc = auc(x_values, y_values)\n",
    "            auc_list.append(area_uc)\n",
    "            acc_list.append(balanced_accuracy_score(y, np.round(prediction)))\n",
    "\n",
    "        #print(f'{model_type} {method} AUCs: ', end=\"\")\n",
    "        #print(['{0:0.2f}'.format(auc) for auc in auc_list])\n",
    "        mean_y = np.mean(y_value_list, axis=0)\n",
    "        mean_auc = auc(mean_x, mean_y) \n",
    "        std_auc = np.std(auc_list) \n",
    "        plt.plot(mean_x, mean_y, lw=2, alpha=.8,\n",
    "                 label=label + r' (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc * 100, std_auc * 100)\n",
    "                 + r' (ACC = %0.2f $\\pm$ %0.2f)' % (np.mean(acc_list) * 100, np.std(acc_list) * 100))\n",
    "\n",
    "        std_y = np.std(y_value_list, axis=0)\n",
    "        upper_y = np.minimum(mean_y + std_y, 1)\n",
    "        lower_y = np.maximum(mean_y - std_y, 0)\n",
    "        plt.fill_between(mean_x, lower_y, upper_y, color='grey', alpha=.2)\n",
    "\n",
    "        \n",
    "    \n",
    "    title = f'{method} for {target_str}'\n",
    "    if plot_roc:\n",
    "        xlabel = 'False Positive Rate'\n",
    "        ylabel = 'True Positive Rate'\n",
    "        plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Chance')\n",
    "    else:\n",
    "        xlabel = 'Recall'\n",
    "        ylabel = 'Precision'\n",
    "        chance = len(y[y==1])/len(y)\n",
    "        plt.plot([0, 1], [chance, chance], linestyle='--', lw=2, color='r', label='Chance N_Positive/N_All', alpha=.8)\n",
    "    plt.ylim(0,1.01)\n",
    "    plt.xlim(0,1.01)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    if save_fig:\n",
    "        plt.savefig(f'{figure_dir}/{target}_{title}.{figure_format}',\n",
    "                    format=figure_format, dpi=300, facecolor='white', transparent=True, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "def compare_NN_RNN(model_types, plot_labels, mimic_version, target, time_step_id, seeds, n_folds, target_indeces):\n",
    "    assert len(model_types) == len(plot_labels)\n",
    "    \n",
    "    model_dict_time = get_model_dict(model_types, mimic_version, target, time_step_id, seeds, n_folds)\n",
    "    print(model_dict_time.keys())\n",
    "    print_param_count(model_dict_time, False)\n",
    "    \n",
    "    input_data_dict = get_pickle_dict('test_data', seeds, mimic_version, target, time_step_id)\n",
    "    predictions = get_prediction_dict(model_dict_time, list(input_data_dict.values())[0])\n",
    "    del input_data_dict\n",
    "    \n",
    "    target_dict = get_pickle_dict('test_targets', seeds, mimic_version, target, time_step_id)\n",
    "    mask_dict = get_pickle_dict('test_targets_mask', seeds, mimic_version, target, time_step_id)\n",
    "\n",
    "    for target_index in target_indeces:\n",
    "        if target_index is not None:\n",
    "            target_str = target[target_index]\n",
    "        else:\n",
    "            target_str = target\n",
    "        plot_curves(predictions, True, target_dict, mask_dict, plot_labels, target_str, target_index=target_index)\n",
    "        plot_curves(predictions, False, target_dict, mask_dict, plot_labels, target_str, target_index=target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_types = ['partial_attention_LSTM', 'comparison_LSTM', 'comparison_FNN', 'comparison_LR']\n",
    "plot_labels = ['Deepak LSTM', 'LSTM', 'NN', 'Logistic Regression']\n",
    "\n",
    "if len(targets) == 1:\n",
    "    model_types.append('random_forest_xgb')\n",
    "    plot_labels.append('Random Forest')\n",
    "\n",
    "compare_NN_RNN(model_types, plot_labels, mimic_version, targets, time_step_id, seeds, n_folds, target_indices)\n",
    "\n",
    "if len(targets) > 1:\n",
    "    model_types.append('random_forest_xgb')\n",
    "    plot_labels.append('Random Forest')\n",
    "    for i in range(len(targets)):\n",
    "        target = targets[i]\n",
    "        #compare_NN_RNN(model_types, plot_labels, mimic_version, [target], time_step_id, seeds, n_folds, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out which Window Size yields the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dict_mts(time_step_ids, targets, n_folds, model_type='partial_attention_LSTM'):\n",
    "    model_dict_mts = dict()\n",
    "\n",
    "    for time_step_id in time_step_ids:\n",
    "        model_dict_mts[time_step_id] = []\n",
    "        for seed in seeds:\n",
    "            for fold in range(n_folds):\n",
    "                model_name = get_model_name(model_type, mimic_version, targets, time_step_id, seed, fold)\n",
    "                model = load_model(model_name)\n",
    "                model.eval()\n",
    "                model_dict_mts[time_step_id].append(model)\n",
    "        print(f'Loaded {len(model_dict_mts[time_step_id])} models of {time_step_id}')\n",
    "        \n",
    "    return model_dict_mts\n",
    "\n",
    "def get_prediction_dict_multiple_data(time_step_ids, model_dict, seed, mimic_version, target):\n",
    "    prediction_dict = dict()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(model_dict.keys())):\n",
    "            model_type = list(model_dict.keys())[i]\n",
    "            prediction_dict[model_type] = []\n",
    "            folder = get_pickle_folder(mimic_version, time_step_ids[i], seed)\n",
    "            input_data = torch.tensor(load_pickle_file(\"test_data\", target, folder), dtype=torch.float).clone().detach()    \n",
    "            for model in model_dict[model_type]:\n",
    "                prediction = model(input_data)\n",
    "                prediction_dict[model_type].append(torch.stack(prediction, dim=2)[:,:,:,0].detach().numpy())\n",
    "            \n",
    "    print(f'Number of models/predictions per model type: {len(prediction_dict[model_type])}')\n",
    "    return prediction_dict\n",
    "\n",
    "\n",
    "def plot_curves_md(predictions, plot_roc, target_dict_list, mask_dict_list, save_fig=True, target_index=None):\n",
    "    if plot_roc:\n",
    "        method = 'ROC curve'\n",
    "    else:\n",
    "        method = 'Precision-Recall curve'\n",
    "    mean_x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.axhline(0, color='black')\n",
    "    plt.axvline(0, color='black')\n",
    "    for model_type, target_dict, mask_dict in zip(predictions.keys(), target_dict_list, mask_dict_list):\n",
    "        y_value_list = []\n",
    "        auc_list = []\n",
    "        n_models = len(predictions[model_type])\n",
    "        for prediction, y, mask in zip(predictions[model_type],\n",
    "                                          list(target_dict.values()) * n_models, list(mask_dict.values()) * n_models):\n",
    "            if target_index is not None:\n",
    "                mask = mask[:,:,target_index]\n",
    "                y = y[:,:,target_index]\n",
    "                prediction = prediction[:,:,target_index]\n",
    "            y = y[~mask]\n",
    "            prediction = prediction[~mask]\n",
    "            if plot_roc:\n",
    "                x_values, y_values, thresholds = roc_curve(y, prediction)\n",
    "            else:\n",
    "                y_values, x_values, thresholds = precision_recall_curve(y,  prediction)\n",
    "                y_values = y_values[::-1]\n",
    "            # plt.plot(x_values, y_values)\n",
    "            y_value_list.append(interp(mean_x, sorted(x_values), y_values))\n",
    "            area_uc = auc(x_values, y_values)\n",
    "            auc_list.append(area_uc)\n",
    "\n",
    "        #print(f'{model_type} {method} AUCs: ', end=\"\")\n",
    "        #print(['{0:0.2f}'.format(auc) for auc in auc_list])\n",
    "        mean_y = np.mean(y_value_list, axis=0)\n",
    "        mean_auc = auc(mean_x, mean_y)\n",
    "        std_auc = np.std(auc_list)\n",
    "        plt.plot(mean_x, mean_y, lw=2, alpha=.8, label=str(24 * 14 // model_type)\n",
    "                 + r'h per time step (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc * 100, std_auc * 100))\n",
    "\n",
    "        std_y = np.std(y_value_list, axis=0)\n",
    "        upper_y = np.minimum(mean_y + std_y, 1)\n",
    "        lower_y = np.maximum(mean_y - std_y, 0)\n",
    "        plt.fill_between(mean_x, lower_y, upper_y, color='grey', alpha=.2)\n",
    "\n",
    "    target_str = get_targets()\n",
    "    if target_index is not None:\n",
    "        target_str = target_str[target_index]\n",
    "    title = f'{method} for {target_str}'\n",
    "    if plot_roc:\n",
    "        xlabel = 'False Positive Rate'\n",
    "        ylabel = 'True Positive Rate'\n",
    "        plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Chance')\n",
    "    else:\n",
    "        xlabel = 'Recall'\n",
    "        ylabel = 'Precision'\n",
    "        chance = len(y[y==1])/len(y)\n",
    "        plt.plot([0, 1], [chance, chance], linestyle='--', lw=2, color='r', label='Chance N_Positive/N_All', alpha=.8)\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    if save_fig:\n",
    "        plt.savefig(f'{figure_dir}/{target}_{title}.{figure_format}',\n",
    "                    format=figure_format, dpi=300, facecolor='white', transparent=True, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def compare_window_size(time_step_ids, mimic_version, target, seeds, n_folds, target_indices):\n",
    "    model_dict_mts = get_model_dict_mts(time_step_ids, target, n_folds)\n",
    "    print(model_dict_mts.keys())\n",
    "    print_param_count(model_dict_mts)\n",
    "    \n",
    "    predictions_md = get_prediction_dict_multiple_data(time_step_ids, model_dict_mts, seeds[0], mimic_version, target)\n",
    "\n",
    "    #Plotting\n",
    "    target_dict_list = []\n",
    "    for time_step_id in time_step_ids:\n",
    "        target_dict_list.append(get_pickle_dict(\"test_targets\", seeds, mimic_version, target, time_step_id))\n",
    "    mask_dict_list = []\n",
    "    for time_step_id in time_step_ids:\n",
    "        mask_dict_list.append(get_pickle_dict(\"test_targets_mask\", seeds, mimic_version, target, time_step_id))\n",
    "    for target_index in target_indices:\n",
    "        plot_curves_md(predictions_md, True, target_dict_list, mask_dict_list, target_index=target_index)\n",
    "        plot_curves_md(predictions_md, False, target_dict_list, mask_dict_list, target_index=target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "second_time_step_id = 28\n",
    "time_step_ids = [time_step_id, second_time_step_id]\n",
    "#compare_window_size(time_step_ids, mimic_version, targets, seeds, n_folds, target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_types = ['partial_attention_LSTM', 'full_attention_LSTM', 'hopfield_layer', 'hopfield_pooling', 'hopfield_lookup',\n",
    "               'partial_hopfield_LSTM', 'full_hopfield_LSTM']\n",
    "plot_labels = ['Deepak LSTM', 'Attention LSTM', 'Hopfield Layer', 'Hopfield Pooling', 'Hopfield Lookup',\n",
    "               'Hopfield LSTM P','Hopfield LSTM F']\n",
    "\n",
    "if len(targets) == 1:\n",
    "    model_types.append('xgb')\n",
    "    plot_labels.append('XGB')\n",
    "\n",
    "compare_NN_RNN(model_types, plot_labels, mimic_version, targets, time_step_id, seeds, n_folds, target_indices)\n",
    "\n",
    "if len(targets) > 1:\n",
    "    model_types.append('xgb')\n",
    "    plot_labels.append('XGB')\n",
    "    for i in range(len(targets)):\n",
    "        target = targets[i]\n",
    "        #compare_NN_RNN(model_types, plot_labels, mimic_version, [target], time_step_id, seeds, n_folds, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_attention_dict(model_dict, input_data, mask):\n",
    "    input_data = torch.tensor(input_data, dtype=torch.float).clone().detach()\n",
    "    attention_dict = dict()\n",
    "    with torch.no_grad():\n",
    "        for model_type in model_dict.keys():\n",
    "            print(f'Creating Predictions for {model_type=}')\n",
    "            attention_dict[model_type] = []\n",
    "            n_models = len(model_dict[model_type])\n",
    "            for model in model_dict[model_type]:\n",
    "                model(input_data)\n",
    "                attention = model.attention\n",
    "                attention[mask] = np.nan\n",
    "                attention_dict[model_type].append(attention)\n",
    "    print(f'Number of models/attention per model type: {len(attention_dict[model_type])}')\n",
    "    return attention_dict\n",
    "\n",
    "def prep_features(targets, folder):\n",
    "    features = load_pickle_file('features', targets, folder)\n",
    "    # If the timestep feature was added\n",
    "    features = features[1:]\n",
    "    ''' Due to the way features are selectd from the EMR and the fact potassium can be a \n",
    "    delivered medication or a lab value, special care was taken to ensure proper representation on heatmaps '''\n",
    "\n",
    "    if 'digoxin(?!.*fab)' in features:\n",
    "        indexy = features.index('digoxin(?!.*fab)')\n",
    "        features[indexy] = 'digoxin'\n",
    "\n",
    "    if 'potassium_y' in features:\n",
    "        indexy = features.index('potassium_y')\n",
    "        features[indexy] = 'potassium_med'\n",
    "\n",
    "    if 'potassium_x' in features:\n",
    "        indexy = features.index('potassium_x')\n",
    "        features[indexy] = 'potassium'\n",
    "\n",
    "    if 'cipfloxacin' in features:\n",
    "        indexy = features.index('cipfloxacin')\n",
    "        features[indexy] = 'ciprofloxacin'\n",
    "\n",
    "    features = [feature.lower() for feature in features]\n",
    "    \n",
    "    \n",
    "    ## FEATURES BY CATEGORY ##\n",
    "\n",
    "    cbc_diff_features = ['RBCs', 'WBCs', 'platelets', 'hemoglobin', 'hemocrit', 'atypical lymphocytes', 'bands',\n",
    "                         'basophils', 'eosinophils', 'neutrophils', 'lymphocytes', 'monocytes',\n",
    "                         'polymorphonuclear leukocytes']\n",
    "    vital_features = ['temperature (F)', 'heart rate', 'respiratory rate', 'systolic', 'diastolic', 'pulse oximetry']\n",
    "    lab_features = ['troponin', 'HDL', 'LDL', 'BUN', 'INR', 'PTT', 'PT', 'triglycerides', 'creatinine',\n",
    "                      'glucose', 'sodium', 'potassium', 'chloride', 'bicarbonate',\n",
    "                      'blood culture', 'urine culture', 'surface culture', 'sputum' + \n",
    "                      ' culture', 'wound culture', 'Inspired O2 Fraction', 'central venous pressure', \n",
    "                      'PEEP Set', 'tidal volume', 'anion gap']\n",
    "\n",
    "    demographic_features = ['age', 'm', 'black', 'daily weight', 'tobacco', 'diabetes', 'history of CV events']\n",
    "\n",
    "    med_features = ['epoetin', 'warfarin', 'heparin', 'enoxaparin', 'fondaparinux',\n",
    "                                          'asprin', 'ketorolac', 'acetominophen', \n",
    "                                          'insulin', 'glucagon', \n",
    "                                          'potassium_med', 'calcium gluconate', \n",
    "                                          'fentanyl', 'magensium sulfate', \n",
    "                                          'D5W', 'dextrose', \n",
    "                                          'ranitidine', 'ondansetron', 'pantoprazole', 'metoclopramide', \n",
    "                                          'lisinopril', 'captopril', 'statin',  \n",
    "                                          'hydralazine', 'diltiazem', \n",
    "                                          'carvedilol', 'metoprolol', 'labetalol', 'atenolol',\n",
    "                                          'amiodarone', 'digoxin',\n",
    "                                          'clopidogrel', 'nitroprusside', 'nitroglycerin',\n",
    "                                          'vasopressin', 'hydrochlorothiazide', 'furosemide', \n",
    "                                          'atropine', 'neostigmine',\n",
    "                                          'levothyroxine',\n",
    "                                          'oxycodone', 'hydromorphone', 'fentanyl citrate', \n",
    "                                          'tacrolimus', 'prednisone', \n",
    "                                          'phenylephrine', 'norepinephrine',\n",
    "                                          'haloperidol', 'phenytoin', 'trazodone', 'levetiracetam',\n",
    "                                          'diazepam', 'clonazepam',\n",
    "                                          'propofol', 'zolpidem', 'midazolam', \n",
    "                                          'albuterol', 'ipratropium', \n",
    "                                          'diphenhydramine',  \n",
    "                                          '0.9% Sodium Chloride',\n",
    "                                          'phytonadione', \n",
    "                                          'metronidazole', \n",
    "                                          'cefazolin', 'cefepime', 'vancomycin', 'levofloxacin',\n",
    "                                          'ciprofloxacin', 'fluconazole', \n",
    "                                          'meropenem', 'ceftriaxone', 'piperacillin',\n",
    "                                          'ampicillin-sulbactam', 'nafcillin', 'oxacillin',\n",
    "                                          'amoxicillin', 'penicillin', 'SMX-TMP']\n",
    "\n",
    "    cbc_diff_features = [[i.lower(), i.lower()+'_min', i.lower()+'_max', i.lower()+'_std'] for i in cbc_diff_features]\n",
    "    vital_features = [[i.lower(), i.lower()+'_min', i.lower()+'_max', i.lower()+'_std'] for i in vital_features]\n",
    "    lab_features = [[i.lower(), i.lower()+'_min', i.lower()+'_max', i.lower()+'_std'] for i in lab_features]\n",
    "    demographic_features = [i.lower() for i in demographic_features]\n",
    "    med_features = [i.lower() for i in med_features]\n",
    "\n",
    "    cbc_diff_feature_array = np.array(cbc_diff_features).flatten()\n",
    "    vital_features_array = np.array(vital_features).flatten()\n",
    "    lab_features_array = np.array(lab_features).flatten()\n",
    "    demographic_feature_array = np.array(demographic_features).flatten()\n",
    "    med_features_array = np.array(med_features).flatten()\n",
    "\n",
    "    features_built = np.hstack([cbc_diff_feature_array,vital_features_array,\n",
    "                                lab_features_array,demographic_feature_array,med_features_array])\n",
    "\n",
    "    features_built_reduced = [i for i in features_built if i in features]\n",
    "    \n",
    "    ## Identifies the index in the features list in the desired order ##\n",
    "    arranged_indices = [features.index(i) for i in features_built_reduced]\n",
    "    ## This is a sanity check to ensure that features_built_reduced has the same number of elements as our target ##\n",
    "    print(len(features))\n",
    "    print(len(features_built_reduced))\n",
    "    assert len(features) == len(features_built_reduced)\n",
    "    print(list(set(features) - set(features_built_reduced)))\n",
    "    \n",
    "    return features, arranged_indices\n",
    "    \n",
    "def plot_attentions(mean_attentions, targets, folder):\n",
    "    features, arranged_indices = prep_features(targets, folder)\n",
    "    for key in mean_attentions.keys():\n",
    "        activations = mean_attentions[key]\n",
    "        n_time_steps = activations.shape[1]\n",
    "\n",
    "        np.nanargmax(np.nanmean(activations, axis=0).T[arranged_indices], axis=0)\n",
    "        print(np.array(features)[arranged_indices][np.nanargmax(np.nanmean(activations, axis=0).T[arranged_indices], \n",
    "                                                     axis=0)])\n",
    "        print(np.mean(np.nansum(activations, axis=1)))\n",
    "        plt.figure(figsize = (8,20))\n",
    "\n",
    "        sns.set(font_scale = 0.5)\n",
    "\n",
    "        \n",
    "        heatmap(np.nanmean(activations,axis=0).T[arranged_indices], square=False, \n",
    "                yticklabels=np.array(features)[arranged_indices], cmap=my_cmap)\n",
    "        plt.gcf().axes[-1].tick_params(labelsize=10)\n",
    "        plt.xticks(np.arange(n_time_steps)+0.5, np.arange(n_time_steps), fontsize=15)\n",
    "        plt.xlabel('Day', fontsize=15)\n",
    "        plt.ylabel('Features', fontsize=20)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(activations.shape)\n",
    "        if 'partial_attention' in key:\n",
    "            activations = np.nanmean(np.nanmean(activations, axis=2), axis=0)\n",
    "        else:\n",
    "            activations = np.nanmean(np.nanmean(activations, axis=2), axis=0)\n",
    "        print(activations)\n",
    "        plt.figure(figsize = (8,4)) \n",
    "        sns.set(style=\"white\")\n",
    "        y_vector = activations\n",
    "        print('There is a {0} % change between day 0 and 1'.format((y_vector[1] - y_vector[0])/float(y_vector[0])))\n",
    "        color_rank = np.argsort(np.argsort(y_vector))\n",
    "        plot = sns.barplot(x=list(range(len(activations))), y=activations, palette=np.array(color_list)[color_rank])\n",
    "        plt.xlabel('Day', fontsize=15)\n",
    "        plt.ylabel('Feature Activation', fontsize=20)\n",
    "        plt.title(key)\n",
    "        sns.despine()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "def compare_attentions(model_types, plot_labels, mimic_version, target, time_step_id, seeds, n_folds):\n",
    "    assert len(model_types) == len(plot_labels)\n",
    "    \n",
    "    model_dict_time = get_model_dict(model_types, mimic_version, target, time_step_id, seeds, n_folds)\n",
    "    print(model_dict_time.keys())\n",
    "    print_param_count(model_dict_time, False)\n",
    "    \n",
    "    input_data_dict = get_pickle_dict('test_data', seeds, mimic_version, target, time_step_id)\n",
    "    input_data_mask_dict = get_pickle_dict('test_data_mask', seeds, mimic_version, target, time_step_id)\n",
    "    attentions = get_attention_dict(model_dict_time, list(input_data_dict.values())[0], list(input_data_mask_dict.values())[0])\n",
    "    del input_data_dict\n",
    "    del input_data_mask_dict\n",
    "    \n",
    "    mean_attentions = dict()\n",
    "    for key in attentions.keys():\n",
    "        attention = attentions[key]\n",
    "        stacked_attention = np.stack(attention, axis=3)\n",
    "        mean_attentions[key] = np.nanmean(stacked_attention, axis=3)\n",
    "        #print(np.sum(mean_attentions, axis=1))\n",
    "        \n",
    "    plot_attentions(mean_attentions, target, get_pickle_folder(mimic_version, time_step_id, seeds[0]))\n",
    "    \n",
    "model_types = ['partial_attention_LSTM', 'full_attention_LSTM']\n",
    "plot_labels = ['Deepak LSTM', 'Attention LSTM']\n",
    "\n",
    "compare_attentions(model_types, plot_labels, mimic_version, targets, time_step_id, seeds, n_folds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
